# -*- coding: utf-8 -*-
# File Version: v2.6.0
# Project Version: v2.6.0
# Last Updated: 2025-07-05

import os
import json
import csv
import re
from typing import List, Dict, Set
from datetime import datetime
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
from tqdm import tqdm
from dotenv import load_dotenv
from .utils import normalize_subject_name, get_year_from_user

def get_current_year() -> int:
    """ç¾åœ¨ã®å¹´åº¦ã‚’å–å¾—ã™ã‚‹"""
    return datetime.now().year

def clean_subject_name(name: str) -> str:
    """ç§‘ç›®åã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ï¼ˆ[éš”å¹´é–‹è¬›]ãªã©ã‚’å‰Šé™¤ï¼‰"""
    if not name:
        return name
    
    # [éš”å¹´é–‹è¬›]ã‚’å‰Šé™¤
    name = re.sub(r'\[éš”å¹´é–‹è¬›\]', '', name)
    
    # å‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤
    name = name.strip()
    
    return name

def get_csv_files(year: int) -> List[str]:
    """æŒ‡å®šã•ã‚ŒãŸå¹´åº¦ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—ã™ã‚‹ï¼ˆcsvã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚å«ã‚€ï¼‰"""
    base_dir = os.path.join("src", "course_guide", str(year), "csv")
    if not os.path.exists(base_dir):
        raise FileNotFoundError(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {base_dir}")
    
    # csvã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å•ã„åˆã‚ã›
    subdirs = []
    for item in os.listdir(base_dir):
        item_path = os.path.join(base_dir, item)
        if os.path.isdir(item_path):
            subdirs.append(item)
    
    csv_files = []
    
    if subdirs:
        print(f"è¦‹ã¤ã‹ã£ãŸcsvã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {', '.join(subdirs)}")
        while True:
            subdir_input = input("å‡¦ç†ã™ã‚‹csvã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã—ã¦ãã ã•ã„ï¼ˆç©ºã®å ´åˆã¯å…¨ã¦å‡¦ç†ï¼‰: ").strip()
            if not subdir_input:
                # å…¨ã¦ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ãƒ¡ã‚¤ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‡¦ç†
                # ãƒ¡ã‚¤ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆcsvï¼‰ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
                for file in os.listdir(base_dir):
                    if file.endswith('.csv'):
                        csv_files.append(os.path.join(base_dir, file))
                
                # å…¨ã¦ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
                for subdir in subdirs:
                    subdir_path = os.path.join(base_dir, subdir)
                    if os.path.isdir(subdir_path):
                        for file in os.listdir(subdir_path):
                            if file.endswith('.csv'):
                                csv_files.append(os.path.join(subdir_path, file))
                break
            elif subdir_input in subdirs:
                # æŒ‡å®šã•ã‚ŒãŸã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã¿å‡¦ç†
                subdir_path = os.path.join(base_dir, subdir_input)
                if os.path.isdir(subdir_path):
                    for file in os.listdir(subdir_path):
                        if file.endswith('.csv'):
                            csv_files.append(os.path.join(subdir_path, file))
                break
            else:
                print(f"ç„¡åŠ¹ãªã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã™ã€‚æœ‰åŠ¹ãªé¸æŠè‚¢: {', '.join(subdirs)}")
    else:
        # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãªã„å ´åˆã¯ãƒ¡ã‚¤ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã¿å‡¦ç†
        for file in os.listdir(base_dir):
            if file.endswith('.csv'):
                csv_files.append(os.path.join(base_dir, file))
    
    if not csv_files:
        raise FileNotFoundError(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {base_dir}")
    
    return csv_files

def get_db_connection(db_config: Dict[str, str]):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’å–å¾—ã™ã‚‹"""
    # æ¥ç¶šæ–‡å­—åˆ—ã‚’ä½œæˆ
    connection_string = f"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['db']}"
    
    # ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½œæˆ
    engine = create_engine(
        connection_string,
        connect_args={
            'options': '-c client_encoding=utf-8'
        }
    )
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
    Session = sessionmaker(bind=engine)
    session = Session()
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæ™‚ã«ä¸€åº¦ã ã‘æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¨­å®š
    session.execute(text("SET client_encoding TO 'utf-8'"))
    session.commit()
    
    return session

def get_subject_name_id_from_db(session, subject_name: str) -> int:
    """ç§‘ç›®åIDã‚’å–å¾—ã™ã‚‹"""
    try:
        # ç§‘ç›®åã‚’æ­£è¦åŒ–
        normalized_name = normalize_subject_name(subject_name)
        
        query = text("""
            SELECT subject_name_id 
            FROM subject_name 
            WHERE name = :name
            ORDER BY subject_name_id
            LIMIT 1
        """)
        
        result = session.execute(
            query,
            {"name": normalized_name}
        ).first()
        
        if result:
            return result[0]
        return None
            
    except Exception as e:
        tqdm.write(f"ã‚¨ãƒ©ãƒ¼: ç§‘ç›®åIDã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        session.rollback()
        return None

def get_faculty_id_from_db(session, faculty_name: str) -> int:
    """å­¦éƒ¨IDã‚’å–å¾—ã™ã‚‹"""
    try:
        query = text("""
            SELECT faculty_id 
            FROM faculty 
            WHERE faculty_name = :name
            ORDER BY faculty_id
            LIMIT 1
        """)
        
        result = session.execute(
            query,
            {"name": faculty_name}
        ).first()
        
        if result:
            return result[0]
        return None
            
    except Exception as e:
        tqdm.write(f"ã‚¨ãƒ©ãƒ¼: å­¦éƒ¨IDã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        session.rollback()
        return None

def get_subject_id_from_db(session, subject_name_id: int, faculty_id: int, curriculum_year: int) -> int:
    """ç§‘ç›®IDã‚’å–å¾—ã™ã‚‹"""
    try:
        query = text("""
            SELECT subject_id 
            FROM subject 
            WHERE subject_name_id = :subject_name_id
            AND faculty_id = :faculty_id
            AND curriculum_year = :curriculum_year
            ORDER BY subject_id
            LIMIT 1
        """)
        
        result = session.execute(
            query,
            {
                "subject_name_id": subject_name_id,
                "faculty_id": faculty_id,
                "curriculum_year": curriculum_year
            }
        ).first()
        
        if result:
            return result[0]
        return None
            
    except Exception as e:
        tqdm.write(f"ã‚¨ãƒ©ãƒ¼: ç§‘ç›®IDã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        session.rollback()
        return None

def get_attribute_id_from_db(session, attribute_name: str) -> int:
    """å±æ€§IDã‚’å–å¾—ã™ã‚‹"""
    try:
        query = text("""
            SELECT attribute_id 
            FROM subject_attribute 
            WHERE attribute_name = :name
            ORDER BY attribute_id
            LIMIT 1
        """)
        
        result = session.execute(
            query,
            {"name": attribute_name}
        ).first()
        
        if result:
            return result[0]
        else:
            tqdm.write(f"ã‚¨ãƒ©ãƒ¼: å±æ€§ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {attribute_name}")
            return None
            
    except Exception as e:
        tqdm.write(f"ã‚¨ãƒ©ãƒ¼: å±æ€§IDã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        session.rollback()
        return None

def create_warning_csv(year: int, errors: List[Dict]) -> str:
    """ã‚¨ãƒ©ãƒ¼å†…å®¹ã‚’è©³ç´°ã«CSVãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜è¼‰ã™ã‚‹"""
    # è­¦å‘Šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    warning_dir = os.path.join("warning", str(year))
    os.makedirs(warning_dir, exist_ok=True)
    
    # ç¾åœ¨ã®æ—¥æ™‚ã‚’å–å¾—ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
    current_time = datetime.now()
    filename = f"subject_attribute_value_{current_time.strftime('%Y%m%d_%H%M')}.csv"
    output_file = os.path.join(warning_dir, filename)
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’æ›¸ãè¾¼ã¿
    with open(output_file, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ
        writer.writerow([
            'ãƒ•ã‚¡ã‚¤ãƒ«å',
            'è¡Œç•ªå·',
            'ç§‘ç›®å',
            'å­¦éƒ¨èª²ç¨‹',
            'å¹´åº¦',
            'å±æ€§å',
            'å±æ€§å€¤',
            'ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—',
            'ã‚¨ãƒ©ãƒ¼è©³ç´°',
            'æ­£è¦åŒ–å¾Œç§‘ç›®å',
            'å‡¦ç†æ—¥æ™‚'
        ])
        
        # ã‚¨ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿
        for error in errors:
            writer.writerow([
                error.get('file_name', ''),
                error.get('row_number', ''),
                error.get('subject_name', ''),
                error.get('faculty_name', ''),
                error.get('year', ''),
                error.get('attribute_name', ''),
                error.get('attribute_value', ''),
                error.get('error_type', ''),
                error.get('error_detail', ''),
                error.get('normalized_subject_name', ''),
                error.get('processed_at', '')
            ])
    
    return output_file

def extract_subject_attribute_values(csv_file: str, db_config: Dict[str, str], stats: Dict, errors: List[Dict]) -> List[Dict]:
    """CSVã‹ã‚‰ç§‘ç›®å±æ€§å€¤æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹"""
    attribute_values = []
    session = get_db_connection(db_config)
    
    try:
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f, delimiter='\t')
            rows = list(reader)
            stats['total_items'] += len(rows)
            
            for row_idx, row in enumerate(rows, start=2):  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’é™¤ã„ã¦2ã‹ã‚‰é–‹å§‹
                try:
                    # 17_subject.pyã§å‡¦ç†ã—ãŸãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’é™¤å¤–
                    processed_fields = {'ç§‘ç›®å', 'å­¦éƒ¨èª²ç¨‹', 'å¹´åº¦', 'ç§‘ç›®åŒºåˆ†', 'ç§‘ç›®å°åŒºåˆ†', 'å¿…é ˆåº¦'}
                    attribute_fields = set(row.keys()) - processed_fields
                    
                    # ç§‘ç›®åã‚’æ­£è¦åŒ–
                    subject_name = row['ç§‘ç›®å']
                    cleaned_subject_name = clean_subject_name(subject_name)
                    normalized_subject_name = normalize_subject_name(cleaned_subject_name)
                    
                    # ç§‘ç›®åIDã‚’å–å¾—
                    subject_name_id = get_subject_name_id_from_db(session, cleaned_subject_name)
                    if subject_name_id is None:
                        error_info = {
                            'file_name': os.path.basename(csv_file),
                            'row_number': row_idx,
                            'subject_name': subject_name,
                            'faculty_name': row['å­¦éƒ¨èª²ç¨‹'],
                            'year': row['å¹´åº¦'],
                            'attribute_name': '',
                            'attribute_value': '',
                            'normalized_subject_name': normalized_subject_name,
                            'error_type': 'ç§‘ç›®åIDæœªå–å¾—',
                            'error_detail': f'ç§‘ç›®åãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {cleaned_subject_name} (æ­£è¦åŒ–å¾Œ: {normalized_subject_name})',
                            'processed_at': datetime.now().isoformat()
                        }
                        errors.append(error_info)
                        stats['error_items'] += 1
                        error_type = 'ç§‘ç›®åIDæœªå–å¾—'
                        if error_type not in stats['specific_errors']:
                            stats['specific_errors'][error_type] = 0
                        stats['specific_errors'][error_type] += 1
                        continue
                    
                    # å­¦éƒ¨IDã‚’å–å¾—
                    faculty_id = get_faculty_id_from_db(session, row['å­¦éƒ¨èª²ç¨‹'])
                    if faculty_id is None:
                        error_info = {
                            'file_name': os.path.basename(csv_file),
                            'row_number': row_idx,
                            'subject_name': subject_name,
                            'faculty_name': row['å­¦éƒ¨èª²ç¨‹'],
                            'year': row['å¹´åº¦'],
                            'attribute_name': '',
                            'attribute_value': '',
                            'normalized_subject_name': normalized_subject_name,
                            'error_type': 'å­¦éƒ¨IDæœªå–å¾—',
                            'error_detail': f'å­¦éƒ¨ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {row["å­¦éƒ¨èª²ç¨‹"]}',
                            'processed_at': datetime.now().isoformat()
                        }
                        errors.append(error_info)
                        stats['error_items'] += 1
                        error_type = 'å­¦éƒ¨IDæœªå–å¾—'
                        if error_type not in stats['specific_errors']:
                            stats['specific_errors'][error_type] = 0
                        stats['specific_errors'][error_type] += 1
                        continue
                    
                    # ç§‘ç›®IDã‚’å–å¾—
                    subject_id = get_subject_id_from_db(session, subject_name_id, faculty_id, int(row['å¹´åº¦']))
                    if subject_id is None:
                        error_info = {
                            'file_name': os.path.basename(csv_file),
                            'row_number': row_idx,
                            'subject_name': subject_name,
                            'faculty_name': row['å­¦éƒ¨èª²ç¨‹'],
                            'year': row['å¹´åº¦'],
                            'attribute_name': '',
                            'attribute_value': '',
                            'normalized_subject_name': normalized_subject_name,
                            'error_type': 'ç§‘ç›®IDæœªå–å¾—',
                            'error_detail': f'ç§‘ç›®ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {subject_name} (å­¦éƒ¨: {row["å­¦éƒ¨èª²ç¨‹"]}, å¹´åº¦: {row["å¹´åº¦"]})',
                            'processed_at': datetime.now().isoformat()
                        }
                        errors.append(error_info)
                        stats['error_items'] += 1
                        error_type = 'ç§‘ç›®IDæœªå–å¾—'
                        if error_type not in stats['specific_errors']:
                            stats['specific_errors'][error_type] = 0
                        stats['specific_errors'][error_type] += 1
                        continue
                    
                    # å„å±æ€§ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å‡¦ç†
                    for field_name in attribute_fields:
                        value = row[field_name]
                        # NULLå€¤ã€nullå€¤ã€ç©ºæ–‡å­—ã€ç©ºç™½æ–‡å­—ã®ã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                        if value == "NULL" or value == "null" or not value or value.strip() == "":
                            continue
                        
                        # å¯å¤‰é•·é…åˆ—å½¢å¼ã®å€¤ã‚’åˆ†å‰²
                        values_to_process = []
                        if value.startswith('[') and value.endswith(']'):
                            # [A,B,C]å½¢å¼ã®å ´åˆã€ã‚«ãƒ³ãƒã§åˆ†å‰²
                            array_content = value[1:-1]  # [ã¨]ã‚’é™¤å»
                            if array_content.strip():  # ç©ºã®é…åˆ—ã§ãªã„å ´åˆ
                                values_to_process = [item.strip() for item in array_content.split(',') if item.strip()]
                            else:
                                continue  # ç©ºã®é…åˆ—ã¯ã‚¹ã‚­ãƒƒãƒ—
                        else:
                            # é€šå¸¸ã®å€¤ã®å ´åˆ
                            values_to_process = [value]
                        
                        # åˆ†å‰²ã•ã‚ŒãŸå„å€¤ã‚’å‡¦ç†
                        for single_value in values_to_process:
                            # ç©ºã®å€¤ã¯ã‚¹ã‚­ãƒƒãƒ—
                            if not single_value or single_value.strip() == "":
                                continue
                                
                            attribute_id = get_attribute_id_from_db(session, field_name)
                            if attribute_id is None:
                                error_info = {
                                    'file_name': os.path.basename(csv_file),
                                    'row_number': row_idx,
                                    'subject_name': subject_name,
                                    'faculty_name': row['å­¦éƒ¨èª²ç¨‹'],
                                    'year': row['å¹´åº¦'],
                                    'attribute_name': field_name,
                                    'attribute_value': single_value,
                                    'normalized_subject_name': normalized_subject_name,
                                    'error_type': 'å±æ€§IDæœªå–å¾—',
                                    'error_detail': f'å±æ€§ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {field_name}',
                                    'processed_at': datetime.now().isoformat()
                                }
                                errors.append(error_info)
                                stats['error_items'] += 1
                                error_type = 'å±æ€§IDæœªå–å¾—'
                                if error_type not in stats['specific_errors']:
                                    stats['specific_errors'][error_type] = 0
                                stats['specific_errors'][error_type] += 1
                                continue
                            
                            attribute_value_info = {
                                'subject_id': subject_id,
                                'attribute_id': attribute_id,
                                'value': single_value,
                                'created_at': datetime.now().isoformat()
                            }
                            attribute_values.append(attribute_value_info)
                            stats['valid_items'] += 1
                        
                except Exception as e:
                    error_info = {
                        'file_name': os.path.basename(csv_file),
                        'row_number': row_idx,
                        'subject_name': row.get('ç§‘ç›®å', ''),
                        'faculty_name': row.get('å­¦éƒ¨èª²ç¨‹', ''),
                        'year': row.get('å¹´åº¦', ''),
                        'attribute_name': '',
                        'attribute_value': '',
                        'normalized_subject_name': '',
                        'error_type': 'ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼',
                        'error_detail': str(e),
                        'processed_at': datetime.now().isoformat()
                    }
                    errors.append(error_info)
                    stats['error_items'] += 1
                    error_type = 'ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼'
                    if error_type not in stats['specific_errors']:
                        stats['specific_errors'][error_type] = 0
                    stats['specific_errors'][error_type] += 1
                    tqdm.write(f"ã‚¨ãƒ©ãƒ¼: è¡Œã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                    continue
    finally:
        session.close()
    
    return attribute_values

def create_subject_attribute_value_json(attribute_values: List[Dict]) -> str:
    """ç§‘ç›®å±æ€§å€¤æƒ…å ±ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚‹"""
    output_dir = os.path.join("updates", "subject_attribute_value", "add")
    os.makedirs(output_dir, exist_ok=True)
    
    # ç¾åœ¨ã®æ—¥æ™‚ã‚’å–å¾—ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
    current_time = datetime.now()
    filename = f"subject_attribute_value_{current_time.strftime('%Y%m%d_%H%M')}.json"
    output_file = os.path.join(output_dir, filename)
    
    data = {
        "subject_attribute_values": [{
            "subject_id": value["subject_id"],
            "attribute_id": value["attribute_id"],
            "value": value["value"],
            "created_at": value["created_at"]
        } for value in sorted(attribute_values, key=lambda x: (
            x["subject_id"],
            x["attribute_id"]
        ))]
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return output_file

def main(db_config: Dict[str, str]):
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    try:
        # å¹´åº¦ã®å–å¾—
        year = get_year_from_user()
        
        # çµ±è¨ˆæƒ…å ±ã®åˆæœŸåŒ–
        stats = {
            'total_files': 0,
            'processed_files': 0,
            'total_items': 0,
            'valid_items': 0,
            'error_items': 0,
            'specific_errors': {}
        }
        
        # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã®åé›†ç”¨ãƒªã‚¹ãƒˆ
        errors = []
        
        # å‡¦ç†é–‹å§‹æ™‚ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        tqdm.write(f"\n{'='*60}")
        tqdm.write(f"ç§‘ç›®å±æ€§å€¤ãƒ‘ãƒ¼ã‚µãƒ¼ - å¯¾è±¡å¹´åº¦: {year}")
        tqdm.write(f"{'='*60}")
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å–å¾—
        csv_files = get_csv_files(year)
        stats['total_files'] = len(csv_files)
        tqdm.write(f"å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(csv_files)}")
        
        # ç§‘ç›®å±æ€§å€¤æƒ…å ±ã®æŠ½å‡º
        all_attribute_values = []
        for csv_file in tqdm(csv_files, desc="CSVãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­", unit="file"):
            try:
                attribute_values = extract_subject_attribute_values(csv_file, db_config, stats, errors)
                all_attribute_values.extend(attribute_values)
                stats['processed_files'] += 1
                tqdm.write(f"ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(csv_file)}: {len(attribute_values)}ä»¶ã®å±æ€§å€¤ã‚’æŠ½å‡º")
            except Exception as e:
                tqdm.write(f"ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(csv_file)} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                continue
        
        # é‡è¤‡ã‚’é™¤å»
        unique_attribute_values = []
        seen_values = set()
        for value in all_attribute_values:
            value_key = (value['subject_id'], value['attribute_id'], value['value'])
            if value_key not in seen_values:
                seen_values.add(value_key)
                unique_attribute_values.append(value)
        
        # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
        if errors:
            warning_file = create_warning_csv(year, errors)
            tqdm.write(f"âš ï¸  ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ã—ã¾ã—ãŸ: {warning_file}")
        
        # æœ€çµ‚çµ±è¨ˆã®è¡¨ç¤º
        tqdm.write("\n" + "="*60)
        tqdm.write("å‡¦ç†å®Œäº† - çµ±è¨ˆæƒ…å ±")
        tqdm.write("="*60)
        tqdm.write(f"ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['total_files']}")
        tqdm.write(f"å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['processed_files']}")
        tqdm.write(f"ç·ãƒ‡ãƒ¼ã‚¿æ•°: {stats['total_items']}")
        tqdm.write(f"æ­£å¸¸ãƒ‡ãƒ¼ã‚¿æ•°: {stats['valid_items']}")
        tqdm.write(f"ã‚¨ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿æ•°: {stats['error_items']}")
        
        # ç‰¹å®šã‚¨ãƒ©ãƒ¼ã®è©³ç´°è¡¨ç¤º
        if stats['specific_errors']:
            tqdm.write("\nã‚¨ãƒ©ãƒ¼è©³ç´°:")
            for error_type, count in stats['specific_errors'].items():
                tqdm.write(f"  {error_type}: {count}ä»¶")
        
        tqdm.write("="*60)
        
        # çµæœã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º
        tqdm.write(f"\n{'='*60}")
        tqdm.write("ğŸ“Š æŠ½å‡ºçµæœã‚µãƒãƒªãƒ¼")
        tqdm.write(f"{'='*60}")
        tqdm.write(f"âœ… æ­£å¸¸ãƒ‡ãƒ¼ã‚¿: {len(unique_attribute_values)}ä»¶")
        tqdm.write(f"âš ï¸  ã‚¨ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿: {stats['error_items']}ä»¶")
        tqdm.write(f"ğŸ“ˆ åˆè¨ˆ: {stats['total_items']}ä»¶")
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
        output_file = create_subject_attribute_value_json(unique_attribute_values)
        tqdm.write(f"JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ: {output_file}")
        
    except Exception as e:
        tqdm.write(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        raise

if __name__ == "__main__":
    # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã‚€
    load_dotenv()
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®šã‚’å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®šï¼‰
    db_config = {
        'user': os.getenv('POSTGRES_USER', 'postgres'),
        'password': os.getenv('POSTGRES_PASSWORD', 'postgres'),
        'host': os.getenv('POSTGRES_HOST', 'localhost'),
        'port': os.getenv('POSTGRES_PORT', '5432'),
        'db': os.getenv('POSTGRES_DB', 'syllabus_db')
    }
    
    main(db_config) 